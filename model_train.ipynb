{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict, load_dataset, load_from_disk, Sequence, Value\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "import evaluate\n",
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from typing import Optional, Dict\n",
    "from torch import FloatTensor, tensor\n",
    "import logging\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import hamming_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "Will be training 3 datasets and comparing performances:\n",
    "1. goEmotions + other datasets + textattack data augmentation\n",
    "2. goEmotions + other datasets\n",
    "3. original goEmotions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring',\n",
    "    'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval',\n",
    "    'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief',\n",
    "    'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral'\n",
    "]\n",
    "\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load base model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilbert/distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilbert/distilroberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=len(labels),     \n",
    "    problem_type=\"multi_label_classification\", # uses BCEWithLogitsLoss by default)\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding will be dynamically done in batching with DataCollatorWithPadding\n",
    "def tokenize_func(examples):\n",
    "    return tokenizer(examples['text'], truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets\n",
    "Will be training 3 different datasets\n",
    "- goEmotions (go)\n",
    "- goEmotions + other datasets (merged)\n",
    "- goEmotions + other datasets + textattack data augmentation (augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 88944\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 10426\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 12721\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# augmented\n",
    "dataset = load_from_disk('./datasets/cleaned_hf/augmented_hf')\n",
    "# # merged\n",
    "# dataset = load_from_disk('./datasets/cleaned_hf/merged_hf')\n",
    "# # go\n",
    "# dataset = load_from_disk('./datasets/cleaned_hf/goEmotions_cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset['train']['labels'][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f03f69a095c4abc9c2ce9ca3f8c77dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/88944 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb1273614e104015b1281761b07f07f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/10426 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5e90fb2c88b480487ac5e74803cdd45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/12721 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# In order to use BCEWithLogitsLoss, we need to convert labels to float or it'll give errors\n",
    "# https://github.com/NielsRogge/Transformers-Tutorials/blob/master/BERT/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb\n",
    "# https://discuss.pytorch.org/t/multi-label-binary-classification-result-type-float-cant-be-cast-to-the-desired-output-type-long/117915/3\n",
    "dataset = dataset.cast_column('labels', Sequence(feature=Value(dtype='float32')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset['train']['labels'][1][6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I'm sorry to hear, come see me when you die.\",\n",
       " 'i feel terrified because even if i have the time to write out how i feel about mr',\n",
       " 'I asked at the Bodies Reveled show if they used prisoner bodies.. I got an awkward no.',\n",
       " 'i feel awful for making this all about me and my flawed academia instilled value system but my brain won t shut up about it',\n",
       " 'i feel so repressed when compared to dear a href http eurodancemix']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# before shuffle\n",
    "dataset['train']['text'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle training set\n",
    "dataset['train'] = dataset['train'].shuffle(seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I gave up trying to be 'normal' years ago. But I get your point though.\",\n",
       " '@blue_north27 http://twitpic.com/4jcjr - Mmm yummy... looks like an invitation to me',\n",
       " 'I feel a connection to this woman',\n",
       " 'My friends are awesome! @JNBlack @koreantomcruise -- and the non Twitter ones here right now too!!',\n",
       " 'People like you is also why no players want to play and stay in Orlando']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after shuffle\n",
    "dataset['train']['text'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_class_weights import generate_class_weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.7691456243514355,\n",
       " 1: 1.3645066273932254,\n",
       " 2: 0.49946091644204854,\n",
       " 3: 1.2860613071139386,\n",
       " 4: 1.0808341029504691,\n",
       " 5: 1.779591836734694,\n",
       " 6: 1.401840877569033,\n",
       " 7: 1.4498272152311404,\n",
       " 8: 2.988308023115173,\n",
       " 9: 1.549547038327526,\n",
       " 10: 1.5710046629927936,\n",
       " 11: 0.9356616873553545,\n",
       " 12: 6.0391091797935905,\n",
       " 13: 2.286948472693613,\n",
       " 14: 0.8414758751182593,\n",
       " 15: 1.1933025652033917,\n",
       " 16: 25.210884353741495,\n",
       " 17: 0.3419344917730278,\n",
       " 18: 0.4463357353620105,\n",
       " 19: 12.078218359587181,\n",
       " 20: 0.8910438789821679,\n",
       " 21: 19.488168273444348,\n",
       " 22: 1.7916364515349288,\n",
       " 23: 1.1817602040816326,\n",
       " 24: 3.397402597402597,\n",
       " 25: 0.26198527245949926,\n",
       " 26: 0.8437108708025043,\n",
       " 27: 0.15054127427948574}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = generate_class_weights(dataset['train']['labels'], multi_class=False, one_hot_encoded=True)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2356f6033dd845da8cda2ebd19603fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/88944 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd95d93da51b43b6af78fa67dbae0bc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10426 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7670f72395e24cdba044ddc21b0e9dbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12721 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 88944\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 10426\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 12721\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize in batch\n",
    "tokenized_dataset = dataset.map(tokenize_func, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching and Dynamic padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "- Will be using `Trainer` instead of `SFTTrainer` because `SFTTrainer` is often for llms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batching and dynamic padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    './models/',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute metrics function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metrics\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # # returns dictionary like this: {'precision': 1.0}\n",
    "    # precision_result = precision.compute(predictions=predictions, references=labels)\n",
    "    # recall_result = recall.compute(predictions=predictions, references=labels)\n",
    "    # f1_weighted = f1.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "    # f1_micro = f1.compute(predictions=predictions, references=labels, average=\"micro\")\n",
    "    # f1_macro = f1.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "    \n",
    "\n",
    "    # return {\n",
    "    #     \"precision\": precision_result[\"precision\"],\n",
    "    #     \"recall\": recall_result[\"recall\"],\n",
    "    #     \"f1_weighted\": f1_weighted[\"f1\"],\n",
    "    #     \"f1_micro\": f1_micro[\"f1\"],\n",
    "    #     \"f1_macro\": f1_macro[\"f1\"]\n",
    "    # }\n",
    "\n",
    "    # Generate classification report\n",
    "    report = classification_report(\n",
    "        y_true=labels,\n",
    "        y_pred=predictions,\n",
    "        output_dict=True,  # Convert report to a dictionary\n",
    "        zero_division=0  # Avoid division errors for missing labels\n",
    "    )\n",
    "\n",
    "    # Extract key metrics\n",
    "    micro_precision = report[\"micro avg\"][\"precision\"]\n",
    "    micro_recall = report[\"micro avg\"][\"recall\"]\n",
    "    micro_f1 = report[\"micro avg\"][\"f1-score\"]\n",
    "\n",
    "    macro_precision = report[\"macro avg\"][\"precision\"]\n",
    "    macro_recall = report[\"macro avg\"][\"recall\"]\n",
    "    macro_f1 = report[\"macro avg\"][\"f1-score\"]\n",
    "\n",
    "    weighted_precision = report[\"weighted avg\"][\"precision\"]\n",
    "    weighted_recall = report[\"weighted avg\"][\"recall\"]\n",
    "    weighted_f1 = report[\"weighted avg\"][\"f1-score\"]\n",
    "\n",
    "    # Combine all metrics into a dictionary\n",
    "    metrics = {\n",
    "        \"micro_precision\": micro_precision,\n",
    "        \"micro_recall\": micro_recall,\n",
    "        \"micro_f1\": micro_f1,\n",
    "        \"macro_precision\": macro_precision,\n",
    "        \"macro_recall\": macro_recall,\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"weighted_precision\": weighted_precision,\n",
    "        \"weighted_recall\": weighted_recall,\n",
    "        \"weighted_f1\": weighted_f1\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Class Weighting Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://discuss.huggingface.co/t/mullti-label-text-classification/44233/3\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights: Optional[Dict[int, float]] = None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        if class_weights is not None:\n",
    "            # dict --> FloatTensor\n",
    "            if isinstance(class_weights, dict):\n",
    "                # Convert to list of values and then to FloatTensor\n",
    "                class_weights = tensor(list(class_weights.values()), dtype=torch.float)\n",
    "                logging.info(f\"Converted class_weights to FloatTensor: {class_weights}\")\n",
    "            elif not isinstance(class_weights, FloatTensor):\n",
    "                raise ValueError(\"class_weights must be a dict or a FloatTensor\")\n",
    "            class_weights = class_weights.to(self.args.device)\n",
    "\n",
    "        self.loss_fct = BCEWithLogitsLoss(pos_weight=class_weights)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"\n",
    "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
    "        Subclass and override for custom behavior.\n",
    "        \"\"\"\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        try:\n",
    "            loss = self.loss_fct(outputs.logits.view(-1, model.num_labels), labels.view(-1, model.num_labels))\n",
    "        except AttributeError:  # DataParallel\n",
    "            loss = self.loss_fct(outputs.logits.view(-1, model.module.num_labels), labels.view(-1, model.num_labels))\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = WeightedTrainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.predict(tokenized_dataset['test'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
